{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e798508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed42c3",
   "metadata": {},
   "source": [
    "# Hybrid Vertical Federated Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class HybridSplitBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.deeper_conv = nn.Sequential(\n",
    "            #nn.AdaptiveMaxPool2d(1)\n",
    "            nn.AdaptiveAvgPool2d(1)  # [B, 128, 1, 1]\n",
    "        )\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(64, 49)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.deeper_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.project(x) \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class LocalHybridSplitClassifierHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(49, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  \n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class GlobalHybridSplitClassifierHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*49, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_concat):\n",
    "        return self.classifier(x_concat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ef4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections.abc import Iterable\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import os \n",
    "\n",
    "\n",
    "from federated_inference.common.environment import Member\n",
    "from federated_inference.configs.data_config import DataConfiguration\n",
    "from federated_inference.configs.model_configs import  HybridSplitModelConfiguration\n",
    "\n",
    "\n",
    "class HybridSplitClient():\n",
    "\n",
    "    def __init__(self, \n",
    "            idx, \n",
    "            seed: int,\n",
    "            data_config: DataConfiguration,\n",
    "            model_config: HybridSplitModelConfiguration,\n",
    "            dataset: TorchDataset,\n",
    "            labels,\n",
    "            log: bool = True, \n",
    "            log_interval: int = 100, \n",
    "            save_interval: int = 10\n",
    "        ):\n",
    "        self.idx = idx\n",
    "        self.seed = seed\n",
    "        self.data = dataset\n",
    "        self.data_config = data_config\n",
    "        self.model_config = model_config\n",
    "        self.device = model_config.DEVICE\n",
    "        self.labels = labels\n",
    "        self.numerical_labels = range(len(labels))\n",
    "        self.member_type = Member.CLIENT\n",
    "        self.model = None\n",
    "        self.log = log\n",
    "        self.log_interval = log_interval\n",
    "        self.save_interval = save_interval\n",
    "        self.base_model = model_config.CLIENT_BASE_MODEL().to(self.device)\n",
    "        self.classifier_model = model_config.CLIENT_CLASSIFIER_MODEL().to(self.device)\n",
    "\n",
    "    def select_subset(self, ids: Iterable[int], set_type: str = \"train\"):\n",
    "        if set_type == \"test\":\n",
    "            return Subset(self.data.test_dataset, ids)\n",
    "        else: \n",
    "            return Subset(self.data.train_dataset, ids)\n",
    "\n",
    "    def send_all(self):\n",
    "        return self.data.train_dataset\n",
    "\n",
    "    def request_pred(self, idx: int|None = None, set_type: str = \"test\", pred_all: bool= False, keep_label: bool = False): \n",
    "        if idx != None:\n",
    "            if set_type == \"test\":\n",
    "                return self.data.test_dataset[idx] if keep_label else self.data.test_dataset[idx][0] \n",
    "        elif pred_all:\n",
    "            return self.data.test_dataset if keep_label else [img for img, label in self.data.test_dataset]\n",
    "\n",
    "    def check(self, predicted_labels, pred_all: bool = True):\n",
    "        if pred_all:\n",
    "            true_labels = self.data.test_dataset.targets\n",
    "            accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "            precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "            recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "            f1 = f1_score(true_labels, predicted_labels, average='macro') \n",
    "\n",
    "            print(\"\\n=== Metrics ===\")\n",
    "            print(f\"Accuracy : {accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall   : {recall:.4f}\")\n",
    "            print(f\"F1 Score : {f1:.4f}\") \n",
    "\n",
    "            cm = confusion_matrix(true_labels, predicted_labels, labels=self.numerical_labels)\n",
    "            self.cm = pd.DataFrame(cm, index=[f'True {l}' for l in self.labels],\n",
    "                                    columns=[f'Pred {l}' for l in self.labels])\n",
    "            self.accuracy = accuracy\n",
    "            self.precision = precision\n",
    "            self.recall = recall\n",
    "            self.f1 = f1 \n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        result_path = f\"./results/hybrid/{self.model_config.version}/{self.data_config.DATASET_NAME}/{self.seed}\"\n",
    "        client_result_path = os.path.join(result_path, \"clients\") \n",
    "\n",
    "        # Base\n",
    "        model_path = os.path.join(client_result_path, f'model_client_base_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        network_state_dict = torch.load(model_path)\n",
    "        self.base = self.base_model.load_state_dict(network_state_dict)\n",
    "\n",
    "        # Classifier Head\n",
    "        model_path = os.path.join(client_result_path, f'model_client_classifier_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        network_state_dict = torch.load(model_path)\n",
    "        self.classifier_model.load_state_dict(network_state_dict)\n",
    "#\n",
    "\n",
    "    def to_loader(self, testdata):\n",
    "        self.testloader = DataLoader(self.data.test_dataset, batch_size=self.model_config.BATCH_SIZE_TEST, shuffle=False) \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from federated_inference.common.environment import Member\n",
    "from federated_inference.common.early_stopper import EarlyStopper\n",
    "\n",
    "class HybridSplitServer():\n",
    "\n",
    "    def __init__(self, \n",
    "            idx: int, \n",
    "            seed: int,\n",
    "            model_config,\n",
    "            data_config,\n",
    "            log: bool = True, \n",
    "            log_interval: int = 100,\n",
    "            save_interval: int = 20\n",
    "        ):\n",
    "        self.idx = idx\n",
    "        self.seed = seed\n",
    "        self.version = model_config.version\n",
    "        self.model_config = model_config\n",
    "        self.data_config = data_config\n",
    "        self.seed = seed\n",
    "        self.number_of_clients = 4\n",
    "        self.n_epoch = model_config.N_EPOCH\n",
    "        self.device = model_config.DEVICE\n",
    "        self.member_type = Member.SERVER\n",
    "        self.server_model = model_config.SERVER_MODEL().to(self.device)\n",
    "        self.client_base_models = [model_config.CLIENT_BASE_MODEL().to(self.device) for c in range(self.number_of_clients)]\n",
    "        self.client_classifier_models = [model_config.CLIENT_CLASSIFIER_MODEL().to(self.device) for c in range(self.number_of_clients)]\n",
    "        self.CRITERION = nn.CrossEntropyLoss()\n",
    "        self.CRITERION_NAME = \"CrossEntropyLoss\"\n",
    "        self.weight_decay = 0\n",
    "        self.SERVER_OPTIMIZER = optim.Adam(self.server_model.parameters() , lr=model_config.LEARNING_RATE, weight_decay=self.weight_decay)\n",
    "        self.CLIENT_BASE_OPTIMIZERS =  [optim.Adam(self.client_base_models[c].parameters() , lr=model_config.LEARNING_RATE, weight_decay=self.weight_decay ) for c in range(self.number_of_clients)]\n",
    "        self.CLIENT_CLASSIFIER_OPTIMIZERS =  [optim.Adam(self.client_classifier_models[c].parameters() , lr=model_config.LEARNING_RATE, weight_decay=self.weight_decay) for c in range(self.number_of_clients)]\n",
    "        self.OPTIMIZER_NAME = \"Adam\"\n",
    "        self.LOCAL_CLASSIFIER_CRITERION = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.log = log\n",
    "        self.log_interval = log_interval\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "        if self.log: \n",
    "            self.train_losses = []\n",
    "            self.test_losses = []\n",
    "            self.accuracies = []\n",
    "\n",
    "\n",
    "    def _to_loader(self, trainsets, testsets, batch_size_train, batch_size_val, batch_size_test, train_shuffle, val_shuffle, test_shuffle, train_ratio):\n",
    "        # TODO refactoing to use self\n",
    "        if True:\n",
    "            dataset_length = len(trainsets[0])\n",
    "            assert all(len(trainset) == dataset_length for trainset in trainsets), \"All trainsets must be the same length\"\n",
    "\n",
    "            indices = np.arange(dataset_length)\n",
    "            self.train_set_indices = np.arange(dataset_length)\n",
    "\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            train_end = round(train_ratio * dataset_length)\n",
    "            \n",
    "            \n",
    "            self.train_indices = indices[:train_end]\n",
    "            val_indices = indices[train_end:]\n",
    "\n",
    "            traindatas = [Subset(trainset, self.train_indices) for trainset in trainsets]\n",
    "            valdatas = [Subset(trainset, val_indices) for trainset in trainsets]\n",
    "        else:\n",
    "            traindatas = [Subset(trainset, range(round(train_ratio*len(trainset)))) for trainset in trainsets]\n",
    "            valdatas = [Subset(trainset, range(round(train_ratio*len(trainset)), len(trainset))) for trainset in trainsets]\n",
    "        self.trainloader = [DataLoader(traindata, batch_size=batch_size_train, shuffle=False)  for traindata in traindatas]\n",
    "        self.valloader = [DataLoader(valdata, batch_size=batch_size_val, shuffle=False) for valdata in valdatas]\n",
    "        self.testloader = [DataLoader(testdata, batch_size=batch_size_test, shuffle=False) for testdata in testsets]\n",
    "\n",
    "    def shuffle_loader(self, trainsets, batch_size_train, batch_size_val, train_ratio):\n",
    "        # TODO refactoing to use self\n",
    "        if True:\n",
    "            dataset_length = len(trainsets[0])\n",
    "            assert all(len(trainset) == dataset_length for trainset in trainsets), \"All trainsets must be the same length\"\n",
    "\n",
    "            np.random.shuffle(self.train_set_indices)\n",
    "\n",
    "            train_end = round(train_ratio * dataset_length)\n",
    "            train_indices = self.train_set_indices[:train_end]\n",
    "            val_indices = self.train_set_indices[train_end:]\n",
    "\n",
    "            traindatas = [Subset(trainset, train_indices) for trainset in trainsets]\n",
    "            valdatas = [Subset(trainset, val_indices) for trainset in trainsets]\n",
    "        else:\n",
    "            traindatas = [Subset(trainset, range(round(train_ratio*len(trainset)))) for trainset in trainsets]\n",
    "            valdatas = [Subset(trainset, range(round(train_ratio*len(trainset)), len(trainset))) for trainset in trainsets]\n",
    "        self.trainloader = [DataLoader(traindata, batch_size=batch_size_train, shuffle=False)  for traindata in traindatas]\n",
    "        self.valloader = [DataLoader(valdata, batch_size=batch_size_val, shuffle=False) for valdata in valdatas]\n",
    "\n",
    "\n",
    "    def _pred_loader(self, testsets, batch_size_test, test_shuffle):\n",
    "        return  [DataLoader(testdata, batch_size=batch_size_test, shuffle=False) for testdata in testsets]\n",
    "        \n",
    "    \n",
    "    def train(self, epoch):\n",
    "        for batch_idx, batches in enumerate(zip(*self.trainloader)):\n",
    "            data_slices = [batch[0].to(self.device).float() for batch in batches]\n",
    "            target = batches[0][1].to(self.device).long()\n",
    "    \n",
    "            # ====== Base forward passes ======\n",
    "            client_activations = []\n",
    "            classifier_grads_per_client = []\n",
    "            classifier_losses = []\n",
    "            classifier_preds = []\n",
    "    \n",
    "            for data, base_model, classifier_model, optimizer in zip(data_slices, self.client_base_models,  self.client_classifier_models, self.CLIENT_CLASSIFIER_OPTIMIZERS):\n",
    "                base_model.train()\n",
    "                activation = base_model(data)\n",
    "                activation.requires_grad_()\n",
    "                client_activations.append(activation)\n",
    "                classifier_model.train()\n",
    "                optimizer.zero_grad()\n",
    "                classifier_output = classifier_model(activation)\n",
    "                classifier_pred = classifier_output.argmax(dim=1)\n",
    "                loss = self.LOCAL_CLASSIFIER_CRITERION(classifier_output, target)\n",
    "                classifier_losses.append(loss.item())\n",
    "    \n",
    "                grads = torch.autograd.grad(loss, [activation] + list(classifier_model.parameters()), retain_graph=True)\n",
    "                classifier_grads_per_client.append(grads[0])\n",
    "    \n",
    "                for param, grad in zip(classifier_model.parameters(), grads[1:]):\n",
    "                    param.grad = grad\n",
    "                optimizer.step()\n",
    "    \n",
    "                classifier_preds.append(classifier_pred)\n",
    "            concat_activations = torch.cat(client_activations, dim=1)\n",
    "    \n",
    "            # ====== Server forward and backprop======\n",
    "            self.server_model.train()\n",
    "            self.SERVER_OPTIMIZER.zero_grad()\n",
    "            server_output = self.server_model(concat_activations)\n",
    "            server_loss = self.CRITERION(server_output, target)\n",
    "            server_pred = server_output.argmax(dim=1)\n",
    "            server_grads = torch.autograd.grad(server_loss, [concat_activations] + list(self.server_model.parameters()), retain_graph=True)\n",
    "            #Server Backprop\n",
    "            server_concat_activation_grad = server_grads[0]\n",
    "            server_model_grads = server_grads[1:]\n",
    "            \n",
    "            # Apply gradients manually to model parameters\n",
    "            for param, grad in zip(self.server_model.parameters(), server_model_grads):\n",
    "                param.grad = grad  # Set .grad for optimizer\n",
    "    \n",
    "            self.SERVER_OPTIMIZER.step()\n",
    "    \n",
    "\n",
    "            # ====== Base Backprop combined gradients ======\n",
    "            activation_sizes = [act.shape[1] for act in client_activations]\n",
    "            activation_grads_server = torch.split(server_concat_activation_grad, activation_sizes, dim=1)\n",
    "    \n",
    "            for i, (base_model, optimizer, data) in enumerate(zip(self.client_base_models, self.CLIENT_BASE_OPTIMIZERS, data_slices)):\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # Combine gradients: server + classifier\n",
    "                combined_grad = activation_grads_server[i] + classifier_grads_per_client[i] \n",
    "    \n",
    "                # Backward through base\n",
    "                activation = base_model(data)\n",
    "                activation.backward(combined_grad)\n",
    "                optimizer.step()\n",
    "            \n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                print(f\"Epoch {epoch} | Batch {batch_idx} | Server Loss: {server_loss.item():.4f} | \"\n",
    "                      f\"Classifier Losses: {[round(x, 4) for x in classifier_losses]}\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.server_model.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*self.valloader)):\n",
    "                # batches is tuple of batch from each loader\n",
    "                data_slices = [batch[0].to(self.device).float() for batch in batches]\n",
    "                target = batches[0][1].to(self.device).long()\n",
    "                client_activations = []\n",
    "                client_val_loss = 0\n",
    "                for data, client_base_model, classifier_model in zip(data_slices, self.client_base_models, self.client_classifier_models):\n",
    "                    #Client FeedForward\n",
    "                    client_base_model.eval()\n",
    "                    activation = client_base_model(data)\n",
    "                    client_activations.append(activation)\n",
    "                    classifier_output = classifier_model(activation)\n",
    "                    client_loss = self.LOCAL_CLASSIFIER_CRITERION(classifier_output, target).item()\n",
    "                    client_val_loss += client_loss\n",
    "                client_val_loss /= self.number_of_clients\n",
    "                concat_activations = torch.cat(client_activations, dim=1)\n",
    "                #Server FeedForward\n",
    "                self.server_model.eval()\n",
    "                output = self.server_model(concat_activations)\n",
    "                val_loss +=  self.CRITERION(output, target).item()\n",
    "\n",
    "        val_loss /= len(self.valloader[0].dataset)\n",
    "\n",
    "        if self.early_stopper.best_loss is None or val_loss < self.early_stopper.best_loss:\n",
    "            print(\"Validation loss improved. Saving model...\")\n",
    "            self.save()\n",
    "        self.early_stopper(val_loss)\n",
    "\n",
    "    def test(self):\n",
    "        self.server_model.eval()\n",
    "        \n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*self.testloader)):\n",
    "                # batches is tuple of batch from each loader\n",
    "                data_slices = [batch[0].to(self.device).float() for batch in batches]\n",
    "                target = batches[0][1].to(self.device).long()\n",
    "                client_activations = []\n",
    "                for data, client_base_model in zip(data_slices, self.client_base_models):\n",
    "                    #Client FeedForward\n",
    "                    client_base_model.eval()\n",
    "                    activation = client_base_model(data)\n",
    "                    client_activations.append(activation)\n",
    "                concat_activations = torch.cat(client_activations, dim=1)\n",
    "                #Server FeedForward\n",
    "                self.server_model.eval()\n",
    "                output = self.server_model(concat_activations)\n",
    "                test_loss +=  self.CRITERION(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.testloader[0].dataset)\n",
    "        accuracy = 100. * correct / len(self.testloader[0].dataset)\n",
    "        if self.log: \n",
    "            self.test_losses.append(test_loss)\n",
    "            self.accuracies.append(accuracy)\n",
    "        print(f'\\nTest set: Average loss per Sample: {test_loss:.4f}, Accuracy: {correct}/{len(self.testloader[0].dataset)} '\n",
    "            f'({accuracy:.0f}%)\\n')\n",
    "            \n",
    "        \n",
    "    def test_inferences(self):\n",
    "        self.server_model.eval()\n",
    "        clients_preds = {i: [] for i in range(self.number_of_clients)}\n",
    "        test_loss = 0\n",
    "        server_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*self.testloaders)):\n",
    "                # batches is tuple of batch from each loader\n",
    "                data_slices = [batch[0].to(self.device).float() for batch in batches]\n",
    "                target = batches[0][1].to(self.device).long()\n",
    "                client_activations = []\n",
    "                for i, (data, client_base_model, classifier_model) in enumerate(zip(data_slices, self.client_base_models, self.client_classifier_models)):\n",
    "                    #Client FeedForward\n",
    "                    client_base_model.eval()\n",
    "                    classifier_model.eval()\n",
    "                    activation = client_base_model(data)\n",
    "                    client_class_logits = classifier_model(activation)\n",
    "                    clients_preds[i] += client_class_logits.argmax(dim=1, keepdim=True).squeeze().tolist()\n",
    "                    client_activations.append(activation)\n",
    "                    \n",
    "                concat_activations = torch.cat(client_activations, dim=1)\n",
    "                #Server FeedForward\n",
    "                self.server_model.eval()\n",
    "                output = self.server_model(concat_activations)\n",
    "                test_loss +=  self.CRITERION(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                server_preds += pred.squeeze().tolist()\n",
    "\n",
    "        return clients_preds, server_preds\n",
    "            \n",
    "    def run_training(self, trainset, testset):\n",
    "        self.server_early_stopper = EarlyStopper()\n",
    "        self.early_stopper = EarlyStopper()\n",
    "        self._to_loader(trainset, testset, \n",
    "            self.model_config.BATCH_SIZE_TRAIN,\n",
    "            self.model_config.BATCH_SIZE_VAL, \n",
    "            self.model_config.BATCH_SIZE_TEST, \n",
    "            self.model_config.TRAIN_SHUFFLE,\n",
    "            self.model_config.VAL_SHUFFLE, \n",
    "            self.model_config.TEST_SHUFFLE,\n",
    "            self.model_config.TRAIN_RATIO)\n",
    "        self.test()\n",
    "        \n",
    "        self.classifier_test()\n",
    "        for epoch in range(1, self.model_config.N_EPOCH + 1):\n",
    "            self.train(epoch)\n",
    "            self.test()\n",
    "            self.classifier_test()\n",
    "            self.validate()\n",
    "            \n",
    "            if self.early_stopper.early_stop:\n",
    "                self.early_stop_epoch = epoch\n",
    "                print(\"early_stop_triggered\")\n",
    "                break\n",
    "            self.shuffle_loader(trainset, \n",
    "            self.model_config.BATCH_SIZE_TRAIN,\n",
    "            self.model_config.BATCH_SIZE_VAL, \n",
    "            self.model_config.TRAIN_RATIO)\n",
    "        \n",
    "        print(\"loading best model to server...\")\n",
    "        self.load()\n",
    "\n",
    "    \n",
    "    def run_infernces(self, testsets): \n",
    "        self.testloaders = [DataLoader(testdata, batch_size=self.model_config.BATCH_SIZE_TEST, shuffle=False) for testdata in testsets]\n",
    "        clients_preds, server_preds = self.test_inferences()\n",
    "\n",
    "        return clients_preds, server_preds\n",
    "        \n",
    "    def pred(self, testset,  pred=True):\n",
    "        predictions = []\n",
    "        loader = self._pred_loader(testset, self.model_config.BATCH_SIZE_TEST, self.model_config.TEST_SHUFFLE)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*loader)):\n",
    "                # batches is tuple of batch from each loader\n",
    "                data_slices = [batch[0].to(self.device).float() for batch in batches]\n",
    "                target = batches[0][1].to(self.device).long()\n",
    "                client_activations = []\n",
    "                for data, client_base_model in zip(data_slices, self.client_base_models):\n",
    "                    #Client FeedForward\n",
    "                    client_base_model.eval()\n",
    "                    activation = client_base_model(data)\n",
    "                    client_activations.append(activation)\n",
    "                concat_activations = torch.cat(client_activations, dim=1)\n",
    "                #Server FeedForward\n",
    "                self.server_model.eval()\n",
    "                output = self.server_model(concat_activations)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                predictions = predictions + pred.squeeze().tolist()\n",
    "        return predictions\n",
    "\n",
    "    def save(self):\n",
    "        result_path = f\"./results/hybrid/{self.model_config.version}/{self.data_config.DATASET_NAME}/{self.seed}\"\n",
    "        os.makedirs(result_path, exist_ok=True)\n",
    "        model_path = os.path.join(result_path, f'model_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        optimizer_path = os.path.join(result_path, f'optimizer_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        torch.save(self.server_model.state_dict(), model_path)\n",
    "        torch.save(self.SERVER_OPTIMIZER.state_dict(), optimizer_path)\n",
    "\n",
    "        client_result_path =os.path.join(result_path, \"clients\")\n",
    "        os.makedirs(client_result_path, exist_ok=True)\n",
    "        for i, client_base_model in enumerate(self.client_base_models): \n",
    "            model_path = os.path.join(client_result_path, f'model_client_base_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            optimizer_path = os.path.join(client_result_path, f'optimizer_client_base_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            torch.save(client_base_model.state_dict(), model_path)\n",
    "            torch.save(self.CLIENT_BASE_OPTIMIZERS[i].state_dict(), optimizer_path)\n",
    "                    \n",
    "        for i, client_classifier_model in enumerate(self.client_classifier_models): \n",
    "            model_path = os.path.join(client_result_path, f'model_client_classifier_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            optimizer_path = os.path.join(client_result_path, f'optimizer_classifier_client_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            torch.save(client_classifier_model.state_dict(), model_path)\n",
    "            torch.save(self.CLIENT_CLASSIFIER_OPTIMIZERS[i].state_dict(), optimizer_path)\n",
    "        \n",
    "    def load(self):\n",
    "        result_path = f\"./results/hybrid/{self.model_config.version}/{self.data_config.DATASET_NAME}/{self.seed}\"\n",
    "        model_path = os.path.join(result_path, f'model_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        optimizer_path = os.path.join(result_path, f'optimizer_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        network_state_dict = torch.load(model_path)\n",
    "        self.server_model.load_state_dict(network_state_dict)\n",
    "        optimizer_state_dict = torch.load(optimizer_path)\n",
    "        self.SERVER_OPTIMIZER.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "        client_result_path = os.path.join(result_path, \"clients\")\n",
    "        for i, (client_base_model, model_optimizer) in enumerate(zip(self.client_base_models, self.CLIENT_BASE_OPTIMIZERS)): \n",
    "            model_path = os.path.join(client_result_path, f'model_client_base_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            optimizer_path = os.path.join(client_result_path, f'optimizer_client_base_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            network_state_dict = torch.load(model_path)\n",
    "            client_base_model.load_state_dict(network_state_dict)\n",
    "            optimizer_state_dict = torch.load(optimizer_path)\n",
    "            model_optimizer.load_state_dict(optimizer_state_dict)\n",
    "        \n",
    "        for i, (client_classifier_model, model_optimizer) in enumerate(zip(self.client_classifier_models, self.CLIENT_CLASSIFIER_OPTIMIZERS)): \n",
    "            model_path = os.path.join(client_result_path, f'model_client_classifier_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            optimizer_path = os.path.join(client_result_path, f'optimizer_classifier_client_{i}.pth').replace(\"\\\\\", \"/\")\n",
    "            network_state_dict = torch.load(model_path)\n",
    "            client_classifier_model.load_state_dict(network_state_dict)\n",
    "            optimizer_state_dict = torch.load(optimizer_path)\n",
    "            model_optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "                   \n",
    "                            \n",
    "    def classifier_test(self):\n",
    "        test_loss_values = [0 for c in self.client_base_models]\n",
    "        correct_values = [0 for c in self.client_base_models]\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*self.testloader)):\n",
    "                data_slices = [batch[0].to(self.device).float() for batch in batches]\n",
    "                target = batches[0][1].to(self.device).long()\n",
    "                for client_idx, client in enumerate(zip(data_slices, self.client_base_models, self.client_classifier_models)):\n",
    "                    data, client_base_model, classifier_model = client \n",
    "                    client_base_model.eval()\n",
    "                    classifier_model.eval()\n",
    "                    activation = client_base_model(data)\n",
    "                    classifier_output = classifier_model(activation)\n",
    "                    test_loss_values[client_idx] +=  self.LOCAL_CLASSIFIER_CRITERION(classifier_output, target).item()\n",
    "                    pred = classifier_output.argmax(dim=1, keepdim=True)\n",
    "                    correct_values[client_idx]+= pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss_values = [t/ len(self.testloader[0].dataset) for t in test_loss_values]\n",
    "        accuracy_values = [100. * c / len(self.testloader[0].dataset) for c in correct_values]\n",
    "\n",
    "        if self.log: \n",
    "            self.test_losses.append(test_loss_values)\n",
    "            self.accuracies.append(accuracy_values)\n",
    "        print(f'\\nTest set: Average loss per Sample: {test_loss_values}, Accuracy: {correct_values}/{len(self.testloader[0].dataset)}'\n",
    "            f'({accuracy_values}%)\\n')\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a9b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "from federated_inference.common.environment import  DataMode, TransformType\n",
    "from federated_inference.configs.model_configs import HybridVFLWoRouterModelConfiguration\n",
    "from federated_inference.simulations.simulation import Simulation\n",
    "from federated_inference.simulations.utils import *\n",
    "\n",
    "class HybridSplitSimulation(Simulation): \n",
    "    def __init__(self, seed, version, data_config, transform_config, server_model, client_base_model, client_classifier_model, transform_type: TransformType = TransformType.FULL_STRIDE_PARTITION, exist=False):\n",
    "        self.seed = seed\n",
    "        self.data_config = data_config\n",
    "        self.transform_config = transform_config\n",
    "        self.server_model_config =  HybridVFLWoRouterModelConfiguration(version, server_model, client_base_model, client_classifier_model)\n",
    "        self.data_mode = DataMode.VERTICAL\n",
    "        self.transform_type = transform_type\n",
    "        self.dataset =  self.load_data(data_config)\n",
    "        self.client_datasets, self.transformation = self.transform_data(self.dataset, data_mode = self.data_mode, transform_config = transform_config, transform_type = self.transform_type)\n",
    "        self.clients = [HybridSplitClient(idx, seed, data_config, self.server_model_config, dataset, data_config.LABELS) for idx, dataset in enumerate(self.client_datasets)]\n",
    "        self.server = HybridSplitServer(0, seed, self.server_model_config , self.data_config)\n",
    "\n",
    "    def train(self): \n",
    "        datasets = [client.send_all() for client in self.clients]\n",
    "        testsets = [client.request_pred(pred_all = True, keep_label = True) for client in self.clients]\n",
    "        self.server.run_training(datasets, testsets)\n",
    "\n",
    "    def test_inference(self):\n",
    "        testsets = [client.request_pred(pred_all = True, keep_label = True) for client in self.clients]\n",
    "        predictions = self.server.pred(testsets)\n",
    "        self.clients[0].check(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa21fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training data loaded.\n",
      "MNIST test data loaded.\n",
      "\n",
      "Test set: Average loss per Sample: 0.0362, Accuracy: 1327/10000 (13%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.036146217918396, 0.03614617111682892, 0.036166108417510986, 0.036185809755325316], Accuracy: [1028, 1135, 1010, 1010]/10000([10.28, 11.35, 10.1, 10.1]%)\n",
      "\n",
      "Epoch 1 | Batch 0 | Server Loss: 2.4625 | Classifier Losses: [2.4107, 2.4103, 2.4235, 2.5071]\n",
      "Epoch 1 | Batch 100 | Server Loss: 0.1976 | Classifier Losses: [1.0592, 0.8775, 1.0187, 1.1039]\n",
      "Epoch 1 | Batch 200 | Server Loss: 0.1362 | Classifier Losses: [1.0058, 0.8088, 0.7607, 1.2128]\n",
      "Epoch 1 | Batch 300 | Server Loss: 0.1888 | Classifier Losses: [1.2131, 0.8384, 0.8045, 0.9768]\n",
      "Epoch 1 | Batch 400 | Server Loss: 0.0758 | Classifier Losses: [0.8404, 0.6219, 0.9676, 0.8837]\n",
      "Epoch 1 | Batch 500 | Server Loss: 0.1291 | Classifier Losses: [1.0472, 0.7902, 0.7493, 1.0545]\n",
      "Epoch 1 | Batch 600 | Server Loss: 0.2153 | Classifier Losses: [1.1443, 0.8018, 0.747, 0.8607]\n",
      "Epoch 1 | Batch 700 | Server Loss: 0.0716 | Classifier Losses: [0.8141, 0.7911, 0.6018, 0.8838]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0013, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.012896945407986641, 0.010039759626984597, 0.00969170889109373, 0.011553734564781188], Accuracy: [6986, 7830, 7767, 7293]/10000([69.86, 78.3, 77.67, 72.93]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 2 | Batch 0 | Server Loss: 0.0652 | Classifier Losses: [0.8325, 0.5017, 0.4841, 1.0441]\n",
      "Epoch 2 | Batch 100 | Server Loss: 0.0874 | Classifier Losses: [0.9911, 0.6799, 0.7356, 0.7071]\n",
      "Epoch 2 | Batch 200 | Server Loss: 0.0958 | Classifier Losses: [0.9356, 0.7166, 0.598, 1.1582]\n",
      "Epoch 2 | Batch 300 | Server Loss: 0.0135 | Classifier Losses: [0.8804, 0.5948, 0.6389, 0.6973]\n",
      "Epoch 2 | Batch 400 | Server Loss: 0.0985 | Classifier Losses: [1.118, 0.6699, 0.7192, 0.6805]\n",
      "Epoch 2 | Batch 500 | Server Loss: 0.0533 | Classifier Losses: [1.1379, 0.9169, 0.7102, 0.8714]\n",
      "Epoch 2 | Batch 600 | Server Loss: 0.2172 | Classifier Losses: [0.9887, 0.8283, 0.7581, 1.0261]\n",
      "Epoch 2 | Batch 700 | Server Loss: 0.1468 | Classifier Losses: [0.9155, 0.7766, 0.4553, 0.7138]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0008, Accuracy: 9823/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.01198113486468792, 0.007786878816783428, 0.008591184529662132, 0.010647497847676277], Accuracy: [7282, 8391, 7965, 7498]/10000([72.82, 83.91, 79.65, 74.98]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 3 | Batch 0 | Server Loss: 0.0763 | Classifier Losses: [0.8031, 0.6196, 0.8089, 0.8527]\n",
      "Epoch 3 | Batch 100 | Server Loss: 0.0387 | Classifier Losses: [0.7454, 0.5934, 0.7036, 0.8287]\n",
      "Epoch 3 | Batch 200 | Server Loss: 0.0574 | Classifier Losses: [0.7779, 0.6836, 0.897, 0.7056]\n",
      "Epoch 3 | Batch 300 | Server Loss: 0.0610 | Classifier Losses: [0.9759, 0.619, 0.5125, 0.6949]\n",
      "Epoch 3 | Batch 400 | Server Loss: 0.0877 | Classifier Losses: [1.1621, 0.8048, 1.0778, 1.2204]\n",
      "Epoch 3 | Batch 500 | Server Loss: 0.0964 | Classifier Losses: [0.9479, 0.591, 0.509, 0.5744]\n",
      "Epoch 3 | Batch 600 | Server Loss: 0.0824 | Classifier Losses: [0.9834, 0.6357, 0.6307, 0.9051]\n",
      "Epoch 3 | Batch 700 | Server Loss: 0.0127 | Classifier Losses: [0.7824, 0.3222, 0.5144, 0.7055]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0008, Accuracy: 9833/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.01149466002881527, 0.007912034524977208, 0.007794625532627106, 0.009870623409748078], Accuracy: [7362, 8316, 8155, 7739]/10000([73.62, 83.16, 81.55, 77.39]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 4 | Batch 0 | Server Loss: 0.0843 | Classifier Losses: [0.6624, 0.949, 0.7408, 0.9605]\n",
      "Epoch 4 | Batch 100 | Server Loss: 0.0793 | Classifier Losses: [1.0182, 0.8937, 0.9498, 1.0179]\n",
      "Epoch 4 | Batch 200 | Server Loss: 0.0199 | Classifier Losses: [0.8071, 0.6728, 0.4304, 0.7312]\n",
      "Epoch 4 | Batch 300 | Server Loss: 0.0390 | Classifier Losses: [0.721, 0.5946, 0.4756, 0.4734]\n",
      "Epoch 4 | Batch 400 | Server Loss: 0.0725 | Classifier Losses: [0.5958, 0.4849, 0.6805, 0.7838]\n",
      "Epoch 4 | Batch 500 | Server Loss: 0.0204 | Classifier Losses: [0.8351, 0.6206, 0.5927, 0.8252]\n",
      "Epoch 4 | Batch 600 | Server Loss: 0.0536 | Classifier Losses: [0.7126, 0.7208, 0.4647, 0.8086]\n",
      "Epoch 4 | Batch 700 | Server Loss: 0.0692 | Classifier Losses: [0.834, 0.5139, 0.6672, 0.7295]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0007, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.011047784614562989, 0.007516987996548414, 0.007593308572471142, 0.010157488250732422], Accuracy: [7466, 8392, 8163, 7532]/10000([74.66, 83.92, 81.63, 75.32]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 5 | Batch 0 | Server Loss: 0.0163 | Classifier Losses: [0.7094, 0.5867, 0.679, 0.8648]\n",
      "Epoch 5 | Batch 100 | Server Loss: 0.0626 | Classifier Losses: [0.7869, 0.405, 0.6688, 0.7929]\n",
      "Epoch 5 | Batch 200 | Server Loss: 0.0681 | Classifier Losses: [0.6961, 0.5869, 0.5062, 0.696]\n",
      "Epoch 5 | Batch 300 | Server Loss: 0.0912 | Classifier Losses: [0.9014, 0.3437, 0.5222, 0.6925]\n",
      "Epoch 5 | Batch 400 | Server Loss: 0.1369 | Classifier Losses: [0.9566, 0.5482, 0.6233, 0.8267]\n",
      "Epoch 5 | Batch 500 | Server Loss: 0.0540 | Classifier Losses: [1.0569, 0.4006, 0.6722, 0.8627]\n",
      "Epoch 5 | Batch 600 | Server Loss: 0.0875 | Classifier Losses: [1.0465, 0.5526, 0.54, 0.4538]\n",
      "Epoch 5 | Batch 700 | Server Loss: 0.0547 | Classifier Losses: [0.5575, 0.2839, 0.4995, 0.6469]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9866/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010973373994231223, 0.007030493903160095, 0.007614712514728308, 0.00943203304708004], Accuracy: [7480, 8547, 8232, 7789]/10000([74.8, 85.47, 82.32, 77.89]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 6 | Batch 0 | Server Loss: 0.0730 | Classifier Losses: [1.1179, 0.7149, 0.784, 0.6436]\n",
      "Epoch 6 | Batch 100 | Server Loss: 0.0710 | Classifier Losses: [0.9035, 0.6152, 0.7454, 0.8]\n",
      "Epoch 6 | Batch 200 | Server Loss: 0.0457 | Classifier Losses: [0.8806, 0.4661, 0.5304, 0.6142]\n",
      "Epoch 6 | Batch 300 | Server Loss: 0.0541 | Classifier Losses: [0.8535, 0.5595, 0.4508, 0.7391]\n",
      "Epoch 6 | Batch 400 | Server Loss: 0.0788 | Classifier Losses: [1.131, 0.5429, 0.6075, 0.8854]\n",
      "Epoch 6 | Batch 500 | Server Loss: 0.0807 | Classifier Losses: [0.9015, 0.527, 0.6909, 0.7577]\n",
      "Epoch 6 | Batch 600 | Server Loss: 0.0242 | Classifier Losses: [0.6533, 0.5486, 0.7754, 0.6679]\n",
      "Epoch 6 | Batch 700 | Server Loss: 0.1269 | Classifier Losses: [1.0382, 0.839, 0.7939, 0.7496]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.01072899464070797, 0.007062990479171276, 0.007449536677449941, 0.009434038370847702], Accuracy: [7549, 8504, 8192, 7793]/10000([75.49, 85.04, 81.92, 77.93]%)\n",
      "\n",
      "Epoch 7 | Batch 0 | Server Loss: 0.1011 | Classifier Losses: [0.7772, 0.6543, 0.6853, 0.7574]\n",
      "Epoch 7 | Batch 100 | Server Loss: 0.0173 | Classifier Losses: [0.76, 0.3518, 0.43, 0.6087]\n",
      "Epoch 7 | Batch 200 | Server Loss: 0.0286 | Classifier Losses: [0.7541, 0.4878, 0.8179, 0.7699]\n",
      "Epoch 7 | Batch 300 | Server Loss: 0.0815 | Classifier Losses: [0.7548, 0.6967, 0.6134, 0.8628]\n",
      "Epoch 7 | Batch 400 | Server Loss: 0.0237 | Classifier Losses: [0.8478, 0.5769, 0.639, 0.9936]\n",
      "Epoch 7 | Batch 500 | Server Loss: 0.0587 | Classifier Losses: [0.752, 0.4893, 0.5658, 0.6066]\n",
      "Epoch 7 | Batch 600 | Server Loss: 0.0447 | Classifier Losses: [1.1387, 0.5135, 0.4846, 0.6445]\n",
      "Epoch 7 | Batch 700 | Server Loss: 0.0125 | Classifier Losses: [0.5919, 0.5495, 0.5072, 0.6549]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010916625013947487, 0.007080722922831774, 0.007883445686101914, 0.009134549549221993], Accuracy: [7489, 8501, 8053, 7873]/10000([74.89, 85.01, 80.53, 78.73]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 8 | Batch 0 | Server Loss: 0.0561 | Classifier Losses: [0.8861, 0.4807, 0.3596, 0.7121]\n",
      "Epoch 8 | Batch 100 | Server Loss: 0.0183 | Classifier Losses: [0.7281, 0.4642, 0.3948, 0.7509]\n",
      "Epoch 8 | Batch 200 | Server Loss: 0.0954 | Classifier Losses: [0.8095, 0.5702, 0.6071, 0.763]\n",
      "Epoch 8 | Batch 300 | Server Loss: 0.0470 | Classifier Losses: [0.7991, 0.773, 0.4463, 0.7957]\n",
      "Epoch 8 | Batch 400 | Server Loss: 0.0776 | Classifier Losses: [0.5514, 0.4918, 0.5877, 0.7167]\n",
      "Epoch 8 | Batch 500 | Server Loss: 0.0118 | Classifier Losses: [0.6688, 0.3436, 0.4427, 0.7811]\n",
      "Epoch 8 | Batch 600 | Server Loss: 0.0192 | Classifier Losses: [0.6987, 0.3543, 0.5294, 0.932]\n",
      "Epoch 8 | Batch 700 | Server Loss: 0.1791 | Classifier Losses: [0.8687, 0.5988, 0.6383, 0.6349]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010658234935998916, 0.006923071017116309, 0.007246394018083811, 0.009253094428777696], Accuracy: [7574, 8577, 8274, 7843]/10000([75.74, 85.77, 82.74, 78.43]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 9 | Batch 0 | Server Loss: 0.0277 | Classifier Losses: [0.7882, 0.5323, 0.4574, 0.654]\n",
      "Epoch 9 | Batch 100 | Server Loss: 0.0443 | Classifier Losses: [0.8724, 0.5022, 0.3706, 0.5524]\n",
      "Epoch 9 | Batch 200 | Server Loss: 0.0950 | Classifier Losses: [0.759, 0.5052, 0.8683, 0.6551]\n",
      "Epoch 9 | Batch 300 | Server Loss: 0.0422 | Classifier Losses: [0.7849, 0.6856, 0.7318, 0.8464]\n",
      "Epoch 9 | Batch 400 | Server Loss: 0.0510 | Classifier Losses: [0.6413, 0.5243, 0.4044, 0.7434]\n",
      "Epoch 9 | Batch 500 | Server Loss: 0.0566 | Classifier Losses: [0.813, 0.6585, 0.4268, 0.8529]\n",
      "Epoch 9 | Batch 600 | Server Loss: 0.0188 | Classifier Losses: [0.4724, 0.7122, 0.3983, 0.7033]\n",
      "Epoch 9 | Batch 700 | Server Loss: 0.0368 | Classifier Losses: [0.9364, 0.3946, 0.5333, 0.9789]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.011219014963507652, 0.006657924402877689, 0.007186062763631344, 0.009063636296987533], Accuracy: [7462, 8588, 8273, 7876]/10000([74.62, 85.88, 82.73, 78.76]%)\n",
      "\n",
      "Epoch 10 | Batch 0 | Server Loss: 0.1427 | Classifier Losses: [1.0538, 0.5052, 0.4517, 0.6737]\n",
      "Epoch 10 | Batch 100 | Server Loss: 0.1007 | Classifier Losses: [0.7487, 0.4664, 0.7127, 0.7041]\n",
      "Epoch 10 | Batch 200 | Server Loss: 0.0202 | Classifier Losses: [0.6365, 0.7091, 0.6778, 0.6238]\n",
      "Epoch 10 | Batch 300 | Server Loss: 0.0915 | Classifier Losses: [0.9179, 0.4047, 0.69, 0.6775]\n",
      "Epoch 10 | Batch 400 | Server Loss: 0.0110 | Classifier Losses: [0.7836, 0.4201, 0.4167, 0.6623]\n",
      "Epoch 10 | Batch 500 | Server Loss: 0.0058 | Classifier Losses: [0.6521, 0.3779, 0.4221, 0.5442]\n",
      "Epoch 10 | Batch 600 | Server Loss: 0.0352 | Classifier Losses: [0.9658, 0.3415, 0.6523, 0.6685]\n",
      "Epoch 10 | Batch 700 | Server Loss: 0.0314 | Classifier Losses: [0.8412, 0.4811, 0.3998, 0.6593]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010488221609592438, 0.006959849102050066, 0.007174923410266638, 0.009012492868304252], Accuracy: [7660, 8559, 8251, 7940]/10000([76.6, 85.59, 82.51, 79.4]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 11 | Batch 0 | Server Loss: 0.0361 | Classifier Losses: [0.7411, 0.7188, 0.5746, 0.7751]\n",
      "Epoch 11 | Batch 100 | Server Loss: 0.0751 | Classifier Losses: [0.8866, 0.3873, 0.3789, 0.727]\n",
      "Epoch 11 | Batch 200 | Server Loss: 0.0378 | Classifier Losses: [1.0625, 0.6423, 0.6927, 0.6244]\n",
      "Epoch 11 | Batch 300 | Server Loss: 0.1382 | Classifier Losses: [0.8816, 0.5204, 0.511, 0.748]\n",
      "Epoch 11 | Batch 400 | Server Loss: 0.0826 | Classifier Losses: [0.7497, 0.4904, 0.6014, 0.6432]\n",
      "Epoch 11 | Batch 500 | Server Loss: 0.0114 | Classifier Losses: [0.5245, 0.4713, 0.5597, 0.9197]\n",
      "Epoch 11 | Batch 600 | Server Loss: 0.0532 | Classifier Losses: [0.8758, 0.4171, 0.652, 0.8687]\n",
      "Epoch 11 | Batch 700 | Server Loss: 0.0361 | Classifier Losses: [0.7967, 0.3138, 0.6025, 0.6798]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010469498732686043, 0.006458047454804182, 0.007015189510583878, 0.008856635200977325], Accuracy: [7646, 8678, 8336, 7913]/10000([76.46, 86.78, 83.36, 79.13]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 12 | Batch 0 | Server Loss: 0.0300 | Classifier Losses: [1.0758, 0.4422, 0.8846, 0.7659]\n",
      "Epoch 12 | Batch 100 | Server Loss: 0.0336 | Classifier Losses: [0.8301, 0.4707, 0.8327, 0.8776]\n",
      "Epoch 12 | Batch 200 | Server Loss: 0.0468 | Classifier Losses: [0.7596, 0.4236, 0.3368, 0.6277]\n",
      "Epoch 12 | Batch 300 | Server Loss: 0.0489 | Classifier Losses: [0.8867, 0.3528, 0.6016, 0.5827]\n",
      "Epoch 12 | Batch 400 | Server Loss: 0.0522 | Classifier Losses: [0.7456, 0.6229, 0.6631, 0.941]\n",
      "Epoch 12 | Batch 500 | Server Loss: 0.0016 | Classifier Losses: [0.7092, 0.271, 0.581, 0.6292]\n",
      "Epoch 12 | Batch 600 | Server Loss: 0.1618 | Classifier Losses: [1.0354, 0.6149, 0.5752, 0.5765]\n",
      "Epoch 12 | Batch 700 | Server Loss: 0.0496 | Classifier Losses: [0.8121, 0.4458, 0.4506, 0.6305]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.01061531164944172, 0.006464770816266537, 0.007139300616830588, 0.00899506030678749], Accuracy: [7618, 8648, 8316, 7921]/10000([76.18, 86.48, 83.16, 79.21]%)\n",
      "\n",
      "Epoch 13 | Batch 0 | Server Loss: 0.0033 | Classifier Losses: [0.5247, 0.4296, 0.385, 0.634]\n",
      "Epoch 13 | Batch 100 | Server Loss: 0.0244 | Classifier Losses: [0.7302, 0.5611, 0.396, 0.6155]\n",
      "Epoch 13 | Batch 200 | Server Loss: 0.0510 | Classifier Losses: [0.6548, 0.5412, 0.4195, 0.6552]\n",
      "Epoch 13 | Batch 300 | Server Loss: 0.0436 | Classifier Losses: [0.7023, 0.4485, 0.5728, 0.6877]\n",
      "Epoch 13 | Batch 400 | Server Loss: 0.0255 | Classifier Losses: [0.9644, 0.5467, 0.4677, 0.6473]\n",
      "Epoch 13 | Batch 500 | Server Loss: 0.0807 | Classifier Losses: [0.5869, 0.7088, 0.8018, 0.7309]\n",
      "Epoch 13 | Batch 600 | Server Loss: 0.0145 | Classifier Losses: [0.6872, 0.4433, 0.4101, 0.5604]\n",
      "Epoch 13 | Batch 700 | Server Loss: 0.0535 | Classifier Losses: [0.7198, 0.5701, 0.4103, 0.6644]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010452618631720543, 0.006402882073819637, 0.00688754858225584, 0.008862784960865974], Accuracy: [7610, 8696, 8339, 7953]/10000([76.1, 86.96, 83.39, 79.53]%)\n",
      "\n",
      "Epoch 14 | Batch 0 | Server Loss: 0.0785 | Classifier Losses: [0.8676, 0.5335, 0.5721, 0.7406]\n",
      "Epoch 14 | Batch 100 | Server Loss: 0.0435 | Classifier Losses: [0.7189, 0.6992, 0.4318, 0.7714]\n",
      "Epoch 14 | Batch 200 | Server Loss: 0.0041 | Classifier Losses: [0.6163, 0.4562, 0.6364, 0.6067]\n",
      "Epoch 14 | Batch 300 | Server Loss: 0.0841 | Classifier Losses: [0.7831, 0.4433, 0.4209, 0.8481]\n",
      "Epoch 14 | Batch 400 | Server Loss: 0.0563 | Classifier Losses: [1.0169, 0.5624, 0.5495, 0.8766]\n",
      "Epoch 14 | Batch 500 | Server Loss: 0.0128 | Classifier Losses: [0.7629, 0.3134, 0.3794, 0.7874]\n",
      "Epoch 14 | Batch 600 | Server Loss: 0.1644 | Classifier Losses: [0.9391, 0.5698, 0.6896, 0.7677]\n",
      "Epoch 14 | Batch 700 | Server Loss: 0.1609 | Classifier Losses: [0.8816, 0.4836, 0.6672, 0.7304]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010660167369246482, 0.006644258640706539, 0.007206009219586849, 0.009437745749950408], Accuracy: [7592, 8637, 8227, 7774]/10000([75.92, 86.37, 82.27, 77.74]%)\n",
      "\n",
      "Epoch 15 | Batch 0 | Server Loss: 0.0559 | Classifier Losses: [0.7069, 0.4144, 0.6107, 0.5965]\n",
      "Epoch 15 | Batch 100 | Server Loss: 0.0501 | Classifier Losses: [0.8871, 0.336, 0.7562, 0.789]\n",
      "Epoch 15 | Batch 200 | Server Loss: 0.0236 | Classifier Losses: [0.665, 0.6746, 0.5494, 0.633]\n",
      "Epoch 15 | Batch 300 | Server Loss: 0.0047 | Classifier Losses: [0.6747, 0.5684, 0.5417, 0.5797]\n",
      "Epoch 15 | Batch 400 | Server Loss: 0.0201 | Classifier Losses: [0.6914, 0.5686, 0.4327, 0.7225]\n",
      "Epoch 15 | Batch 500 | Server Loss: 0.0148 | Classifier Losses: [0.5917, 0.5115, 0.392, 0.5611]\n",
      "Epoch 15 | Batch 600 | Server Loss: 0.1035 | Classifier Losses: [0.9134, 0.5739, 0.5468, 0.7009]\n",
      "Epoch 15 | Batch 700 | Server Loss: 0.0298 | Classifier Losses: [0.6878, 0.3279, 0.5881, 0.5982]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0006, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010495660558342933, 0.0067568545375019315, 0.006774627979844809, 0.00881631669998169], Accuracy: [7598, 8570, 8414, 7959]/10000([75.98, 85.7, 84.14, 79.59]%)\n",
      "\n",
      "Epoch 16 | Batch 0 | Server Loss: 0.0436 | Classifier Losses: [0.6079, 0.3297, 0.4877, 0.5664]\n",
      "Epoch 16 | Batch 100 | Server Loss: 0.0135 | Classifier Losses: [0.9115, 0.7375, 0.5713, 0.9173]\n",
      "Epoch 16 | Batch 200 | Server Loss: 0.0613 | Classifier Losses: [0.8483, 0.4485, 0.5227, 0.6518]\n",
      "Epoch 16 | Batch 300 | Server Loss: 0.0183 | Classifier Losses: [0.7464, 0.4033, 0.8111, 0.5425]\n",
      "Epoch 16 | Batch 400 | Server Loss: 0.0800 | Classifier Losses: [0.6551, 0.5156, 0.3728, 0.6322]\n",
      "Epoch 16 | Batch 500 | Server Loss: 0.0360 | Classifier Losses: [0.7521, 0.5566, 0.5689, 0.7525]\n",
      "Epoch 16 | Batch 600 | Server Loss: 0.0428 | Classifier Losses: [0.7865, 0.5541, 0.546, 0.7385]\n",
      "Epoch 16 | Batch 700 | Server Loss: 0.0049 | Classifier Losses: [0.6699, 0.4475, 0.548, 0.6806]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010346089169383049, 0.006583000588417053, 0.007098678143322468, 0.008769703337550163], Accuracy: [7598, 8605, 8313, 7931]/10000([75.98, 86.05, 83.13, 79.31]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 17 | Batch 0 | Server Loss: 0.0581 | Classifier Losses: [0.6078, 0.5124, 0.4855, 0.6166]\n",
      "Epoch 17 | Batch 100 | Server Loss: 0.0033 | Classifier Losses: [0.8967, 0.3529, 0.5144, 0.6301]\n",
      "Epoch 17 | Batch 200 | Server Loss: 0.0170 | Classifier Losses: [0.8453, 0.3408, 0.4606, 0.6678]\n",
      "Epoch 17 | Batch 300 | Server Loss: 0.0490 | Classifier Losses: [0.82, 0.9012, 0.494, 0.6839]\n",
      "Epoch 17 | Batch 400 | Server Loss: 0.0184 | Classifier Losses: [0.7947, 0.5616, 0.6743, 0.8704]\n",
      "Epoch 17 | Batch 500 | Server Loss: 0.0248 | Classifier Losses: [0.6921, 0.4095, 0.427, 0.645]\n",
      "Epoch 17 | Batch 600 | Server Loss: 0.0214 | Classifier Losses: [1.0231, 0.6908, 0.6496, 0.811]\n",
      "Epoch 17 | Batch 700 | Server Loss: 0.0038 | Classifier Losses: [0.593, 0.255, 0.3882, 0.7321]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0007, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010340493479371072, 0.006441440382599831, 0.007031863732635975, 0.008890772458910942], Accuracy: [7697, 8686, 8352, 7928]/10000([76.97, 86.86, 83.52, 79.28]%)\n",
      "\n",
      "Epoch 18 | Batch 0 | Server Loss: 0.0742 | Classifier Losses: [0.9077, 0.4912, 0.5298, 0.7215]\n",
      "Epoch 18 | Batch 100 | Server Loss: 0.0128 | Classifier Losses: [0.7618, 0.4012, 0.4183, 0.6662]\n",
      "Epoch 18 | Batch 200 | Server Loss: 0.0128 | Classifier Losses: [0.7867, 0.4756, 0.6183, 0.7208]\n",
      "Epoch 18 | Batch 300 | Server Loss: 0.0522 | Classifier Losses: [0.9056, 0.539, 0.6177, 0.6575]\n",
      "Epoch 18 | Batch 400 | Server Loss: 0.0221 | Classifier Losses: [0.7959, 0.5205, 0.476, 0.6399]\n",
      "Epoch 18 | Batch 500 | Server Loss: 0.0382 | Classifier Losses: [0.8182, 0.5394, 0.6133, 0.8149]\n",
      "Epoch 18 | Batch 600 | Server Loss: 0.1315 | Classifier Losses: [0.9761, 0.8042, 0.6745, 0.7322]\n",
      "Epoch 18 | Batch 700 | Server Loss: 0.0525 | Classifier Losses: [0.8191, 0.6842, 0.5607, 0.7779]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010264726054668426, 0.0063047365467995406, 0.006958090610802174, 0.008746589443087578], Accuracy: [7636, 8681, 8321, 7965]/10000([76.36, 86.81, 83.21, 79.65]%)\n",
      "\n",
      "Epoch 19 | Batch 0 | Server Loss: 0.0438 | Classifier Losses: [0.7369, 0.5101, 0.5198, 0.7356]\n",
      "Epoch 19 | Batch 100 | Server Loss: 0.0040 | Classifier Losses: [0.8836, 0.4877, 0.4693, 0.5276]\n",
      "Epoch 19 | Batch 200 | Server Loss: 0.0267 | Classifier Losses: [0.9924, 0.3275, 0.6214, 0.4796]\n",
      "Epoch 19 | Batch 300 | Server Loss: 0.0184 | Classifier Losses: [0.5644, 0.4076, 0.5044, 0.6083]\n",
      "Epoch 19 | Batch 400 | Server Loss: 0.0540 | Classifier Losses: [0.9731, 0.7561, 0.6898, 0.8745]\n",
      "Epoch 19 | Batch 500 | Server Loss: 0.0138 | Classifier Losses: [0.9474, 0.5842, 0.5528, 0.5634]\n",
      "Epoch 19 | Batch 600 | Server Loss: 0.0338 | Classifier Losses: [0.6784, 0.3758, 0.5312, 0.6556]\n",
      "Epoch 19 | Batch 700 | Server Loss: 0.0948 | Classifier Losses: [0.7463, 0.6769, 0.5257, 0.6363]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010097008627653122, 0.006532478589564562, 0.006887802070379257, 0.008927941018342971], Accuracy: [7735, 8599, 8354, 7942]/10000([77.35, 85.99, 83.54, 79.42]%)\n",
      "\n",
      "Epoch 20 | Batch 0 | Server Loss: 0.0047 | Classifier Losses: [0.7888, 0.3173, 0.3733, 0.5092]\n",
      "Epoch 20 | Batch 100 | Server Loss: 0.0132 | Classifier Losses: [0.8617, 0.5582, 0.557, 0.4451]\n",
      "Epoch 20 | Batch 200 | Server Loss: 0.1154 | Classifier Losses: [0.6602, 0.6217, 0.6872, 0.6261]\n",
      "Epoch 20 | Batch 300 | Server Loss: 0.0266 | Classifier Losses: [0.7143, 0.5766, 0.3003, 0.5518]\n",
      "Epoch 20 | Batch 400 | Server Loss: 0.0086 | Classifier Losses: [0.591, 0.4484, 0.4618, 0.5594]\n",
      "Epoch 20 | Batch 500 | Server Loss: 0.0169 | Classifier Losses: [0.6944, 0.5129, 0.5916, 0.8756]\n",
      "Epoch 20 | Batch 600 | Server Loss: 0.0347 | Classifier Losses: [0.5627, 0.4566, 0.6235, 0.6263]\n",
      "Epoch 20 | Batch 700 | Server Loss: 0.0128 | Classifier Losses: [0.8329, 0.4054, 0.6257, 0.6458]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0004, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010188057002425194, 0.006160883034020662, 0.0067983194664120675, 0.008900950063765048], Accuracy: [7674, 8720, 8366, 7867]/10000([76.74, 87.2, 83.66, 78.67]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 21 | Batch 0 | Server Loss: 0.0022 | Classifier Losses: [0.6015, 0.3407, 0.5676, 0.4685]\n",
      "Epoch 21 | Batch 100 | Server Loss: 0.0283 | Classifier Losses: [0.8569, 0.6094, 0.2966, 0.7414]\n",
      "Epoch 21 | Batch 200 | Server Loss: 0.0134 | Classifier Losses: [0.6865, 0.5061, 0.4356, 0.6058]\n",
      "Epoch 21 | Batch 300 | Server Loss: 0.0179 | Classifier Losses: [0.8294, 0.282, 0.5992, 0.687]\n",
      "Epoch 21 | Batch 400 | Server Loss: 0.0134 | Classifier Losses: [0.8461, 0.6174, 0.4514, 0.7283]\n",
      "Epoch 21 | Batch 500 | Server Loss: 0.0170 | Classifier Losses: [0.718, 0.4762, 0.4228, 0.416]\n",
      "Epoch 21 | Batch 600 | Server Loss: 0.0198 | Classifier Losses: [0.8587, 0.541, 0.3653, 0.593]\n",
      "Epoch 21 | Batch 700 | Server Loss: 0.1614 | Classifier Losses: [0.7904, 0.7212, 0.4296, 0.6417]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009990091881155967, 0.006325544118881226, 0.007109705837816, 0.008808784294128418], Accuracy: [7735, 8696, 8291, 7907]/10000([77.35, 86.96, 82.91, 79.07]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 22 | Batch 0 | Server Loss: 0.0602 | Classifier Losses: [0.6134, 0.4609, 0.4854, 0.537]\n",
      "Epoch 22 | Batch 100 | Server Loss: 0.0501 | Classifier Losses: [0.9761, 0.5509, 0.67, 0.5436]\n",
      "Epoch 22 | Batch 200 | Server Loss: 0.0074 | Classifier Losses: [0.8209, 0.4589, 0.4496, 0.478]\n",
      "Epoch 22 | Batch 300 | Server Loss: 0.0373 | Classifier Losses: [0.7974, 0.4902, 0.3594, 0.9136]\n",
      "Epoch 22 | Batch 400 | Server Loss: 0.0324 | Classifier Losses: [0.7844, 0.3879, 0.436, 0.5464]\n",
      "Epoch 22 | Batch 500 | Server Loss: 0.0776 | Classifier Losses: [0.8729, 0.7113, 0.7599, 0.7476]\n",
      "Epoch 22 | Batch 600 | Server Loss: 0.0215 | Classifier Losses: [0.6752, 0.5019, 0.4281, 0.7962]\n",
      "Epoch 22 | Batch 700 | Server Loss: 0.0517 | Classifier Losses: [0.8566, 0.4648, 0.2787, 0.7088]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009904515010118485, 0.006071298955380917, 0.006703371634334326, 0.008729138948023319], Accuracy: [7722, 8748, 8407, 7982]/10000([77.22, 87.48, 84.07, 79.82]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 23 | Batch 0 | Server Loss: 0.0889 | Classifier Losses: [0.6129, 0.398, 0.571, 0.6419]\n",
      "Epoch 23 | Batch 100 | Server Loss: 0.0213 | Classifier Losses: [0.6708, 0.4417, 0.4917, 0.5244]\n",
      "Epoch 23 | Batch 200 | Server Loss: 0.0251 | Classifier Losses: [0.5515, 0.5527, 0.6387, 0.6003]\n",
      "Epoch 23 | Batch 300 | Server Loss: 0.0594 | Classifier Losses: [0.7341, 0.515, 0.4736, 0.6465]\n",
      "Epoch 23 | Batch 400 | Server Loss: 0.0095 | Classifier Losses: [0.7913, 0.4519, 0.5369, 0.7431]\n",
      "Epoch 23 | Batch 500 | Server Loss: 0.0018 | Classifier Losses: [0.6672, 0.3395, 0.3025, 0.6235]\n",
      "Epoch 23 | Batch 600 | Server Loss: 0.0257 | Classifier Losses: [0.6207, 0.3598, 0.4311, 0.5165]\n",
      "Epoch 23 | Batch 700 | Server Loss: 0.0099 | Classifier Losses: [0.9187, 0.4433, 0.7074, 0.6828]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010622347599267959, 0.00620939603485167, 0.006734266709536314, 0.008914390721917152], Accuracy: [7576, 8709, 8393, 7916]/10000([75.76, 87.09, 83.93, 79.16]%)\n",
      "\n",
      "Epoch 24 | Batch 0 | Server Loss: 0.0181 | Classifier Losses: [0.6034, 0.4983, 0.5494, 0.6895]\n",
      "Epoch 24 | Batch 100 | Server Loss: 0.0244 | Classifier Losses: [0.6919, 0.4592, 0.5766, 0.5252]\n",
      "Epoch 24 | Batch 200 | Server Loss: 0.0748 | Classifier Losses: [0.6859, 0.462, 0.4105, 0.5232]\n",
      "Epoch 24 | Batch 300 | Server Loss: 0.0191 | Classifier Losses: [0.8652, 0.4648, 0.4909, 0.6816]\n",
      "Epoch 24 | Batch 400 | Server Loss: 0.0078 | Classifier Losses: [0.7895, 0.3592, 0.3199, 0.5367]\n",
      "Epoch 24 | Batch 500 | Server Loss: 0.0260 | Classifier Losses: [0.7555, 0.7647, 0.4724, 0.7524]\n",
      "Epoch 24 | Batch 600 | Server Loss: 0.0243 | Classifier Losses: [0.6437, 0.4021, 0.5083, 0.5308]\n",
      "Epoch 24 | Batch 700 | Server Loss: 0.0027 | Classifier Losses: [0.5844, 0.3545, 0.3315, 0.78]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010160752454400063, 0.006120166040956974, 0.006800843846797943, 0.008512886075675487], Accuracy: [7709, 8709, 8380, 8015]/10000([77.09, 87.09, 83.8, 80.15]%)\n",
      "\n",
      "Epoch 25 | Batch 0 | Server Loss: 0.0056 | Classifier Losses: [0.6059, 0.4483, 0.351, 0.5541]\n",
      "Epoch 25 | Batch 100 | Server Loss: 0.0641 | Classifier Losses: [1.0473, 0.8794, 0.7421, 0.5867]\n",
      "Epoch 25 | Batch 200 | Server Loss: 0.0383 | Classifier Losses: [0.8825, 0.6072, 0.5248, 0.9267]\n",
      "Epoch 25 | Batch 300 | Server Loss: 0.0594 | Classifier Losses: [0.8856, 0.4738, 0.4846, 0.6545]\n",
      "Epoch 25 | Batch 400 | Server Loss: 0.0339 | Classifier Losses: [0.7763, 0.6812, 0.3321, 0.6861]\n",
      "Epoch 25 | Batch 500 | Server Loss: 0.0379 | Classifier Losses: [0.5579, 0.5052, 0.4188, 0.7225]\n",
      "Epoch 25 | Batch 600 | Server Loss: 0.0062 | Classifier Losses: [0.7787, 0.3822, 0.378, 0.5435]\n",
      "Epoch 25 | Batch 700 | Server Loss: 0.0836 | Classifier Losses: [0.7339, 0.5719, 0.7345, 0.7945]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0004, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009863228031992912, 0.0061729485053569075, 0.006526964341104031, 0.008749696671962738], Accuracy: [7745, 8708, 8459, 7960]/10000([77.45, 87.08, 84.59, 79.6]%)\n",
      "\n",
      "Epoch 26 | Batch 0 | Server Loss: 0.0803 | Classifier Losses: [0.6012, 0.3701, 0.5746, 0.66]\n",
      "Epoch 26 | Batch 100 | Server Loss: 0.0164 | Classifier Losses: [0.9065, 0.4879, 0.5869, 0.6724]\n",
      "Epoch 26 | Batch 200 | Server Loss: 0.0042 | Classifier Losses: [0.7125, 0.432, 0.5716, 0.5484]\n",
      "Epoch 26 | Batch 300 | Server Loss: 0.1107 | Classifier Losses: [0.7421, 0.3994, 0.5067, 0.6442]\n",
      "Epoch 26 | Batch 400 | Server Loss: 0.0147 | Classifier Losses: [0.7076, 0.6015, 0.5419, 0.6728]\n",
      "Epoch 26 | Batch 500 | Server Loss: 0.0660 | Classifier Losses: [0.7766, 0.5878, 0.4353, 0.6111]\n",
      "Epoch 26 | Batch 600 | Server Loss: 0.0048 | Classifier Losses: [0.8425, 0.4587, 0.3674, 0.5223]\n",
      "Epoch 26 | Batch 700 | Server Loss: 0.0327 | Classifier Losses: [0.7508, 0.4447, 0.3911, 0.8348]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009927633768320083, 0.006216489164531231, 0.00677804431989789, 0.008498241221904755], Accuracy: [7738, 8683, 8390, 7988]/10000([77.38, 86.83, 83.9, 79.88]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 27 | Batch 0 | Server Loss: 0.1107 | Classifier Losses: [0.6785, 0.4073, 0.5118, 0.5575]\n",
      "Epoch 27 | Batch 100 | Server Loss: 0.1215 | Classifier Losses: [0.8959, 0.3943, 0.5304, 0.681]\n",
      "Epoch 27 | Batch 200 | Server Loss: 0.0211 | Classifier Losses: [0.9742, 0.4531, 0.6386, 0.7943]\n",
      "Epoch 27 | Batch 300 | Server Loss: 0.0498 | Classifier Losses: [0.637, 0.3332, 0.5209, 0.7189]\n",
      "Epoch 27 | Batch 400 | Server Loss: 0.0221 | Classifier Losses: [0.7072, 0.4683, 0.417, 0.8048]\n",
      "Epoch 27 | Batch 500 | Server Loss: 0.0022 | Classifier Losses: [0.5374, 0.2211, 0.4691, 0.4512]\n",
      "Epoch 27 | Batch 600 | Server Loss: 0.0351 | Classifier Losses: [0.6234, 0.4639, 0.5558, 0.6237]\n",
      "Epoch 27 | Batch 700 | Server Loss: 0.0372 | Classifier Losses: [0.4987, 0.336, 0.4061, 0.6738]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0004, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010157999265193939, 0.006107492806762457, 0.006623398593813181, 0.008451198893785476], Accuracy: [7662, 8732, 8431, 8056]/10000([76.62, 87.32, 84.31, 80.56]%)\n",
      "\n",
      "Validation loss improved. Saving model...\n",
      "Epoch 28 | Batch 0 | Server Loss: 0.0819 | Classifier Losses: [0.6702, 0.5321, 0.4716, 0.7473]\n",
      "Epoch 28 | Batch 100 | Server Loss: 0.0154 | Classifier Losses: [0.669, 0.5146, 0.5434, 0.6571]\n",
      "Epoch 28 | Batch 200 | Server Loss: 0.0981 | Classifier Losses: [0.8999, 0.726, 0.6572, 0.74]\n",
      "Epoch 28 | Batch 300 | Server Loss: 0.0231 | Classifier Losses: [0.9639, 0.3718, 0.7462, 0.6195]\n",
      "Epoch 28 | Batch 400 | Server Loss: 0.0060 | Classifier Losses: [0.6425, 0.3145, 0.5311, 0.4793]\n",
      "Epoch 28 | Batch 500 | Server Loss: 0.0567 | Classifier Losses: [0.8187, 0.5109, 0.498, 0.7442]\n",
      "Epoch 28 | Batch 600 | Server Loss: 0.1046 | Classifier Losses: [0.8233, 0.5761, 0.5168, 0.5935]\n",
      "Epoch 28 | Batch 700 | Server Loss: 0.0295 | Classifier Losses: [0.8592, 0.6367, 0.413, 0.7894]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0004, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009863010594248772, 0.0060806617993861434, 0.006775961719453335, 0.00858730262517929], Accuracy: [7771, 8752, 8362, 8021]/10000([77.71, 87.52, 83.62, 80.21]%)\n",
      "\n",
      "Epoch 29 | Batch 0 | Server Loss: 0.0285 | Classifier Losses: [0.8009, 0.5411, 0.3955, 0.4952]\n",
      "Epoch 29 | Batch 100 | Server Loss: 0.0193 | Classifier Losses: [0.7077, 0.5283, 0.4677, 0.6416]\n",
      "Epoch 29 | Batch 200 | Server Loss: 0.0070 | Classifier Losses: [0.513, 0.4482, 0.3937, 0.7956]\n",
      "Epoch 29 | Batch 300 | Server Loss: 0.1261 | Classifier Losses: [0.7927, 0.6091, 0.4716, 0.7456]\n",
      "Epoch 29 | Batch 400 | Server Loss: 0.0895 | Classifier Losses: [0.8781, 0.7366, 0.665, 0.7771]\n",
      "Epoch 29 | Batch 500 | Server Loss: 0.0050 | Classifier Losses: [0.8891, 0.4071, 0.397, 0.5884]\n",
      "Epoch 29 | Batch 600 | Server Loss: 0.0895 | Classifier Losses: [0.5342, 0.3284, 0.502, 0.7965]\n",
      "Epoch 29 | Batch 700 | Server Loss: 0.1141 | Classifier Losses: [1.0827, 0.6948, 0.6689, 0.6861]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0004, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009846831101179123, 0.006078322161361575, 0.006578935030847788, 0.008406646673381328], Accuracy: [7765, 8735, 8434, 8031]/10000([77.65, 87.35, 84.34, 80.31]%)\n",
      "\n",
      "Epoch 30 | Batch 0 | Server Loss: 0.1087 | Classifier Losses: [0.7992, 0.4559, 0.5747, 0.6948]\n",
      "Epoch 30 | Batch 100 | Server Loss: 0.0227 | Classifier Losses: [0.6116, 0.7815, 0.4384, 0.7324]\n",
      "Epoch 30 | Batch 200 | Server Loss: 0.0353 | Classifier Losses: [0.8129, 0.691, 0.6885, 0.7387]\n",
      "Epoch 30 | Batch 300 | Server Loss: 0.0041 | Classifier Losses: [0.6175, 0.4723, 0.3716, 0.6121]\n",
      "Epoch 30 | Batch 400 | Server Loss: 0.0460 | Classifier Losses: [0.6783, 0.3133, 0.5241, 0.7403]\n",
      "Epoch 30 | Batch 500 | Server Loss: 0.0404 | Classifier Losses: [0.6988, 0.339, 0.7503, 0.4931]\n",
      "Epoch 30 | Batch 600 | Server Loss: 0.0706 | Classifier Losses: [0.6536, 0.4888, 0.5077, 0.498]\n",
      "Epoch 30 | Batch 700 | Server Loss: 0.0068 | Classifier Losses: [0.7038, 0.5095, 0.4307, 0.5353]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.009898254671692848, 0.0061057536024600265, 0.006745492570847273, 0.00854968131929636], Accuracy: [7747, 8724, 8398, 8008]/10000([77.47, 87.24, 83.98, 80.08]%)\n",
      "\n",
      "Epoch 31 | Batch 0 | Server Loss: 0.0268 | Classifier Losses: [0.8152, 0.5623, 0.4391, 0.6397]\n",
      "Epoch 31 | Batch 100 | Server Loss: 0.1850 | Classifier Losses: [1.0282, 0.446, 0.6229, 0.8006]\n",
      "Epoch 31 | Batch 200 | Server Loss: 0.0044 | Classifier Losses: [0.6152, 0.6466, 0.392, 0.7492]\n",
      "Epoch 31 | Batch 300 | Server Loss: 0.0440 | Classifier Losses: [0.6525, 0.4804, 0.3795, 0.6461]\n",
      "Epoch 31 | Batch 400 | Server Loss: 0.0103 | Classifier Losses: [0.7792, 0.4798, 0.4379, 0.6978]\n",
      "Epoch 31 | Batch 500 | Server Loss: 0.0784 | Classifier Losses: [0.7335, 0.4628, 0.6444, 1.0315]\n",
      "Epoch 31 | Batch 600 | Server Loss: 0.0035 | Classifier Losses: [0.655, 0.2291, 0.4423, 0.5916]\n",
      "Epoch 31 | Batch 700 | Server Loss: 0.0433 | Classifier Losses: [0.5748, 0.4891, 0.592, 0.7012]\n",
      "\n",
      "Test set: Average loss per Sample: 0.0005, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "\n",
      "Test set: Average loss per Sample: [0.010109050160646438, 0.00599551075771451, 0.006757844712585211, 0.008421213591098785], Accuracy: [7700, 8756, 8363, 8023]/10000([77.0, 87.56, 83.63, 80.23]%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m transform_config = DataTransformConfiguration()\n\u001b[32m     11\u001b[39m simulation2 = HybridSplitSimulation(seed, VERSION,  data_config, transform_config, GlobalHybridSplitClassifierHead, HybridSplitBase, LocalHybridSplitClassifierHead)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43msimulation2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mHybridSplitSimulation.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     20\u001b[39m datasets = [client.send_all() \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clients]\n\u001b[32m     21\u001b[39m testsets = [client.request_pred(pred_all = \u001b[38;5;28;01mTrue\u001b[39;00m, keep_label = \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clients]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestsets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 294\u001b[39m, in \u001b[36mHybridSplitServer.run_training\u001b[39m\u001b[34m(self, trainset, testset)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.test()\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.classifier_test()\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.early_stopper.early_stop:\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.early_stop_epoch = epoch\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mHybridSplitServer.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    181\u001b[39m val_loss = \u001b[32m0\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# batches is tuple of batch from each loader\u001b[39;49;00m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_slices\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\workspace\\dev\\master\\src\\federated_inference\\transform\\wrapper.py:17\u001b[39m, in \u001b[36mSensorWrapper.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> SensorView:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m   data, target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m   data = \u001b[38;5;28mself\u001b[39m._select_sensorview(data).evidence\n\u001b[32m     19\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform: \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\workspace\\dev\\master\\src\\federated_inference\\transform\\wrapper.py:33\u001b[39m, in \u001b[36mDatasetWrapper.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> SensorView:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m   data, target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     34\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform: \n\u001b[32m     35\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.transform(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    172\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img.to(dtype=default_float_dtype).div(\u001b[32m255\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from federated_inference.common.utils import set_seed\n",
    "from federated_inference.configs.data_config import DataConfiguration\n",
    "from federated_inference.configs.transform_config import DataTransformConfiguration\n",
    "\n",
    "DATASET = 'MNIST'\n",
    "VERSION = \"hybrid_wo_router_v1\"\n",
    "for seed in [4]:\n",
    "    set_seed(seed)\n",
    "    data_config = DataConfiguration(DATASET)\n",
    "    transform_config = DataTransformConfiguration()\n",
    "    simulation2 = HybridSplitSimulation(seed, VERSION,  data_config, transform_config, GlobalHybridSplitClassifierHead, HybridSplitBase, LocalHybridSplitClassifierHead)\n",
    "    simulation2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b1adf",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 11:01:38,932 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.79607844].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you're using PyTorch and simulation is already defined\n",
    "image, label = simulation.dataset.train_dataset[0]\n",
    "image = simulation.client_datasets[0].train_dataset.dataset[0][0][0].evidence\n",
    "\n",
    "# If the image is a tensor, convert it to numpy\n",
    "if hasattr(image, 'numpy'):\n",
    "    image = image.numpy()\n",
    "\n",
    "# If the image has shape [C, H, W], convert it to [H, W, C]\n",
    "if image.ndim == 3 and image.shape[0] in [1, 3]:\n",
    "    image = image.transpose(1, 2, 0)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image, cmap='gray' if image.ndim == 2 or image.shape[2] == 1 else None)\n",
    "plt.title(f'Label: {label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
