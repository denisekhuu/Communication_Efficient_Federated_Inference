{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "779ea875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.4, 0.153, 0.845], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.05, 0.80, 0.55]], # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0a7c72d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "81def715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6899edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_unique_row_binary_tensor(n):\n",
    "    tensor = torch.randint(0, 2, (n, n))\n",
    "\n",
    "    # Keep checking for duplicates until all rows are unique\n",
    "    while True:\n",
    "        # Convert rows to tuples for easy uniqueness check\n",
    "        rows = [tuple(row.tolist()) for row in tensor]\n",
    "\n",
    "        # Find duplicates by counting occurrences\n",
    "        duplicates = [row for row in set(rows) if rows.count(row) > 1]\n",
    "\n",
    "        if not duplicates:\n",
    "            break  # all rows unique\n",
    "\n",
    "        # For each duplicate, randomly pick one occurrence to change\n",
    "        for dup in duplicates:\n",
    "            indices = [i for i, row in enumerate(rows) if row == dup]\n",
    "            # Keep one occurrence intact, change others\n",
    "            for idx in indices[1:]:\n",
    "                tensor[idx] = torch.randint(0, 2, (n,))\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.register_buffer('mask', torch.zeros(context_length, context_length)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_tokens, d_in)\n",
    "        Returns:\n",
    "            context_vec: Tensor of shape (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class OnCloudMNISTModel(nn.Module):\n",
    "    def __init__(self, d_in=196, d_out=64, context_length=4):\n",
    "        super().__init__()\n",
    "        self.name = \"OnCloudMNISTModel\"\n",
    "\n",
    "        # Pass required dimensions\n",
    "        self.features = SelfAttention(d_in, d_out, context_length)\n",
    "\n",
    "        # Classifier assumes output shape: (batch_size, context_length, d_out)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(context_length * d_out, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_length=4, d_in=196)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "    \n",
    "        if self.training:\n",
    "            p = 0.2 \n",
    "            mask = (torch.rand(b, num_tokens, device=x.device) > p).float()\n",
    "            x = x * mask.unsqueeze(-1)  \n",
    "            \n",
    "        if num_tokens < self.features.context_length:\n",
    "            pad_len = self.features.context_length - num_tokens\n",
    "            pad_tensor = torch.zeros(b, pad_len, d_in, device=x.device, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad_tensor], dim=1)  # pad on token dim\n",
    "\n",
    "        x = self.features(x)  # → (batch_size, 4, d_out)\n",
    "        x = x.view(x.size(0), -1)  # Flatten → (batch_size, 4*d_out)\n",
    "        x = self.classifier(x)     # → (batch_size, 10)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa393c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84b151ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0c38377c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (192x196 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[211]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m context_length = batch.shape[\u001b[32m1\u001b[39m]\n\u001b[32m      7\u001b[39m ca = SelfAttention(d_in, d_out, context_length, \u001b[32m0.0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m context_vecs = \u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(context_vecs)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcontext_vecs.shape:\u001b[39m\u001b[33m\"\u001b[39m, context_vecs.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[210]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03mx: Tensor of shape (batch_size, num_tokens, d_in)\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    context_vec: Tensor of shape (batch_size, num_tokens, d_out)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m b, num_tokens, d_in = x.shape\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m queries = \u001b[38;5;28mself\u001b[39m.W_query(x)\n\u001b[32m     54\u001b[39m values = \u001b[38;5;28mself\u001b[39m.W_value(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (192x196 and 3x2)"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = SelfAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4e1f6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Simulate input: batch_size=64, 4 sources, 196 features each\n",
    "batch = torch.randn(64, 3, 196)\n",
    "\n",
    "# Create model\n",
    "model = OnCloudMNISTModel(d_in=196, d_out=64, context_length=4)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(batch)\n",
    "\n",
    "print(\"logits.shape:\", logits.shape)  # → torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bddb88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "from federated_inference.common.environment import  DataMode, TransformType\n",
    "from federated_inference.simulations.simulation import Simulation\n",
    "from federated_inference.simulations.utils import *\n",
    "from federated_inference.configs.model_configs import OnCloudModelConfiguration\n",
    "\n",
    "class OnCloudVerticalSimulation(Simulation): \n",
    "    def __init__(self, seed, version, data_config, transform_config, model, transform_type: TransformType = TransformType.FULL_STRIDE_PARTITION, exist=False):\n",
    "        self.seed = seed\n",
    "        self.version = version\n",
    "        self.data_config = data_config\n",
    "        self.transform_config = transform_config\n",
    "        self.server_model_config = OnCloudModelConfiguration\n",
    "        self.data_mode = DataMode.VERTICAL\n",
    "        self.transform_type = transform_type\n",
    "        self.dataset =  self.load_data(data_config)\n",
    "        self.client_datasets, self.transformation = self.transform_data(self.dataset, data_mode = self.data_mode, transform_config = transform_config, transform_type = self.transform_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "75ba0dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training data loaded.\n",
      "MNIST test data loaded.\n"
     ]
    }
   ],
   "source": [
    "from federated_inference.common.utils import set_seed\n",
    "from federated_inference.configs.data_config import DataConfiguration\n",
    "from federated_inference.configs.transform_config import DataTransformConfiguration\n",
    "DATASET = 'MNIST'\n",
    "VERSION = \"attention_v1\"\n",
    "seed = 4\n",
    "data_config = DataConfiguration(DATASET)\n",
    "transform_config = DataTransformConfiguration()\n",
    "simulation = OnCloudVerticalSimulation(seed, VERSION, data_config, transform_config, OnCloudMNISTModel, exist=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0665751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_batch_columns(batch_list):\n",
    "    \"\"\"\n",
    "    batch_list: list of batches from each dataset loader\n",
    "    Each batch is usually a tuple (inputs, targets)\n",
    "    \"\"\"       \n",
    "    inputs_list = [b[0] for b in batch_list]  # get inputs from each batch\n",
    "    flattened = [x.view(x.size(0), -1)for x in inputs_list]\n",
    "    inputs_tensor = torch.stack(flattened, dim=1)\n",
    "    targets_list = [b[1] for b in batch_list] \n",
    "    combined_targets = targets_list[0] \n",
    "    return inputs_tensor, combined_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1480729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4, 196])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "trainsets =  [client.train_dataset for client in simulation.client_datasets]\n",
    "testsets = [client.test_dataset for client in simulation.client_datasets]\n",
    "testloader = [DataLoader(testdata, batch_size=54, shuffle=False) for testdata in testsets]\n",
    "trainloader = [DataLoader(traindata, batch_size=64, shuffle=False)  for traindata in trainsets]\n",
    "for batch_idx, batches in enumerate(zip(*trainloader)):\n",
    "    inputs_tensor, combined_targets = combine_batch_columns(batches)\n",
    "    print(inputs_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5fc15330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 Loss: 2.4311\n",
      "Epoch 0 Batch 100 Loss: 1.0218\n",
      "Epoch 0 Batch 200 Loss: 0.9316\n",
      "Epoch 0 Batch 300 Loss: 0.8708\n",
      "Epoch 0 Batch 400 Loss: 0.6706\n",
      "Epoch 0 Batch 500 Loss: 0.7131\n",
      "Epoch 0 Batch 600 Loss: 0.9207\n",
      "Epoch 0 Batch 700 Loss: 0.7305\n",
      "Epoch 0 Batch 800 Loss: 0.7004\n",
      "Epoch 0 Batch 900 Loss: 0.8032\n",
      "Epoch 0 Validation Accuracy: 0.8770\n",
      "Epoch 1 Batch 0 Loss: 0.6349\n",
      "Epoch 1 Batch 100 Loss: 0.7741\n",
      "Epoch 1 Batch 200 Loss: 0.6285\n",
      "Epoch 1 Batch 300 Loss: 0.7795\n",
      "Epoch 1 Batch 400 Loss: 0.5425\n",
      "Epoch 1 Batch 500 Loss: 0.5838\n",
      "Epoch 1 Batch 600 Loss: 0.7232\n",
      "Epoch 1 Batch 700 Loss: 0.7094\n",
      "Epoch 1 Batch 800 Loss: 0.8272\n",
      "Epoch 1 Batch 900 Loss: 0.7676\n",
      "Epoch 1 Validation Accuracy: 0.9074\n",
      "Epoch 2 Batch 0 Loss: 0.6425\n",
      "Epoch 2 Batch 100 Loss: 0.5870\n",
      "Epoch 2 Batch 200 Loss: 0.7176\n",
      "Epoch 2 Batch 300 Loss: 0.4657\n",
      "Epoch 2 Batch 400 Loss: 0.6159\n",
      "Epoch 2 Batch 500 Loss: 0.8573\n",
      "Epoch 2 Batch 600 Loss: 0.6005\n",
      "Epoch 2 Batch 700 Loss: 0.6772\n",
      "Epoch 2 Batch 800 Loss: 0.7446\n",
      "Epoch 2 Batch 900 Loss: 0.6402\n",
      "Epoch 2 Validation Accuracy: 0.9181\n",
      "Epoch 3 Batch 0 Loss: 0.4996\n",
      "Epoch 3 Batch 100 Loss: 0.7252\n",
      "Epoch 3 Batch 200 Loss: 0.5554\n",
      "Epoch 3 Batch 300 Loss: 0.7639\n",
      "Epoch 3 Batch 400 Loss: 0.5569\n",
      "Epoch 3 Batch 500 Loss: 0.5721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[218]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     10\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_targets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_batch_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\workspace\\dev\\master\\src\\federated_inference\\transform\\wrapper.py:17\u001b[39m, in \u001b[36mSensorWrapper.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> SensorView:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m   data, target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m   data = \u001b[38;5;28mself\u001b[39m._select_sensorview(data).evidence\n\u001b[32m     19\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform: \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\workspace\\dev\\master\\src\\federated_inference\\transform\\wrapper.py:33\u001b[39m, in \u001b[36mDatasetWrapper.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> SensorView:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m   data, target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     34\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform: \n\u001b[32m     35\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.transform(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\denis\\Documents\\workspace\\env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = OnCloudMNISTModel(d_in=196, d_out=64, context_length=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10  # or whatever you want\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, batches in enumerate(zip(*trainloader)):\n",
    "        inputs_tensor, combined_targets = combine_batch_columns(batches)\n",
    "\n",
    "        outputs = model(inputs_tensor)\n",
    "        loss = criterion(outputs, combined_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {batch_idx} Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for test_batches in zip(*testloader):\n",
    "            inputs_tensor, combined_targets = combine_batch_columns(test_batches)\n",
    "            outputs = model(inputs_tensor)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == combined_targets).sum().item()\n",
    "            total_samples += combined_targets.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Epoch {epoch} Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "from federated_inference.common.early_stopper import EarlyStopper\n",
    "from federated_inference.common.cost_calculator import CostCalculator\n",
    "from federated_inference.common.environment import Member\n",
    "\n",
    "\n",
    "class OnCloudVerticalServer():\n",
    "\n",
    "    def __init__(self, \n",
    "            idx, \n",
    "            seed,\n",
    "            model_config,\n",
    "            data_config,\n",
    "            log: bool = True, \n",
    "            log_interval: int = 100,\n",
    "            save_interval: int = 20\n",
    "        ):\n",
    "        self.idx = idx\n",
    "        self.seed = seed\n",
    "        self.model_config = model_config\n",
    "        self.data_config = data_config\n",
    "        self.seed = seed\n",
    "        self.n_epoch = model_config.N_EPOCH\n",
    "        self.device = model_config.DEVICE\n",
    "        self.member_type = Member.SERVER\n",
    "        self.model = model_config.MODEL\n",
    "        self.optimizer = model_config.OPTIMIZER\n",
    "        self.criterion = model_config.CRITERION\n",
    "        self.costs = []\n",
    "\n",
    "        self.log = log\n",
    "        self.log_interval = log_interval\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "        if self.log: \n",
    "            self.train_losses = []\n",
    "            self.test_losses = []\n",
    "            self.accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "    def _to_loader(self, trainsets, testsets, batch_size_train, batch_size_val, batch_size_test, train_shuffle, val_shuffle, test_shuffle, train_ratio):\n",
    "        # TODO refactoing to use self\n",
    "        if True:\n",
    "            print(\"shuffle training_data\")\n",
    "            # Assuming all trainsets have the same length\n",
    "            dataset_length = len(trainsets[0])\n",
    "            assert all(len(trainset) == dataset_length for trainset in trainsets), \"All trainsets must be the same length\"\n",
    "\n",
    "            self.train_set_indices = np.arange(dataset_length)\n",
    "            \n",
    "            np.random.shuffle(self.train_set_indices)\n",
    "\n",
    "            train_end = round(train_ratio * dataset_length)\n",
    "            train_indices = self.train_set_indices[:train_end]\n",
    "            val_indices = self.train_set_indices[train_end:]\n",
    "\n",
    "            traindatas = [Subset(trainset, train_indices) for trainset in trainsets]\n",
    "            valdatas = [Subset(trainset, val_indices) for trainset in trainsets]\n",
    "        else:\n",
    "            traindatas = [Subset(trainset, range(round(train_ratio*len(trainset)))) for trainset in trainsets]\n",
    "            valdatas = [Subset(trainset, range(round(train_ratio*len(trainset)), len(trainset))) for trainset in trainsets]\n",
    "        self.trainloader = [DataLoader(traindata, batch_size=batch_size_train, shuffle=False)  for traindata in traindatas]\n",
    "        self.valloader = [DataLoader(valdata, batch_size=batch_size_val, shuffle=False) for valdata in valdatas]\n",
    "        self.testloader = [DataLoader(testdata, batch_size=batch_size_test, shuffle=False) for testdata in testsets]\n",
    "\n",
    "\n",
    "    def _pred_loader(self, testsets, batch_size_test, test_shuffle):\n",
    "        return  [DataLoader(testdata, batch_size=batch_size_test, shuffle=False) for testdata in testsets]\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.model.train()\n",
    "        for batch_idx, batches in enumerate(zip(*self.trainloader)):\n",
    "            # batches is tuple of batch from each loader\n",
    "            data, target = self.combine_batch_columns(batches, expect_target=True)\n",
    "            data = data.to(self.device).float()\n",
    "            target = target.to(self.device).long()\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.log and batch_idx % self.log_interval == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(self.trainloader[0].dataset)} '\n",
    "                    f'({100. * batch_idx / len(self.trainloader[0]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "            if batch_idx % self.save_interval == 0:\n",
    "                self.train_losses.append(loss.item())\n",
    "        val_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*self.valloader)):\n",
    "                data, target = self.combine_batch_columns(batches, expect_target=True)\n",
    "                data = data.to(self.device).float()\n",
    "                target = target.to(self.device).long()\n",
    "                output = self.model(data)\n",
    "                val_loss += self.criterion(output, target).item()\n",
    "        val_loss /= len(self.valloader[0].dataset)\n",
    "        if self.early_stopper.best_loss is None or val_loss < self.early_stopper.best_loss:\n",
    "            print(\"Validation loss improved. Saving model...\")\n",
    "            self.save()\n",
    "        self.early_stopper(val_loss)\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batches in enumerate(zip(*self.testloader)):\n",
    "                # batches is tuple of batch from each loader\n",
    "                data, target = self.combine_batch_columns(batches)\n",
    "                data = data.to(self.device).float()\n",
    "                target = target.to(self.device).long()\n",
    "                output = self.model(data)\n",
    "                test_loss += self.criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.testloader[0].dataset)\n",
    "        accuracy = 100. * correct / len(self.testloader[0].dataset)\n",
    "        if self.log: \n",
    "            self.test_losses.append(test_loss)\n",
    "            self.accuracies.append(accuracy)\n",
    "        print(f'\\nTest set: Average loss per sample: {test_loss:.4f}, Accuracy: {correct}/{len(self.testloader[0].dataset)} '\n",
    "            f'({accuracy:.0f}%)\\n')\n",
    "            \n",
    "    def run_training(self, trainset, testset):\n",
    "        self.early_stopper = EarlyStopper()\n",
    "        self._to_loader(trainset, testset, \n",
    "            self.model_config.BATCH_SIZE_TRAIN,\n",
    "            self.model_config.BATCH_SIZE_VAL, \n",
    "            self.model_config.BATCH_SIZE_TEST, \n",
    "            self.model_config.TRAIN_SHUFFLE,\n",
    "            self.model_config.VAL_SHUFFLE, \n",
    "            self.model_config.TEST_SHUFFLE,\n",
    "            self.model_config.TRAIN_RATIO)\n",
    "        self.test()\n",
    "        for epoch in range(1, self.model_config.N_EPOCH + 1):\n",
    "            self.train(epoch)\n",
    "            self.test()\n",
    "\n",
    "    def save(self):\n",
    "        result_path = f\"./results/oncloud/{self.model_config.version}/{self.data_config.DATASET_NAME}/{self.seed}\"\n",
    "        os.makedirs(result_path, exist_ok=True)\n",
    "        model_path = os.path.join(result_path, f'model_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        optimizer_path = os.path.join(result_path, f'optimizer_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        torch.save(self.optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        result_path = f\"./results/oncloud/{self.model_config.version}/{self.data_config.DATASET_NAME}/{self.seed}\"\n",
    "        model_path = os.path.join(result_path, f'model_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        optimizer_path = os.path.join(result_path, f'optimizer_server_{self.idx}.pth').replace(\"\\\\\", \"/\")\n",
    "        network_state_dict = torch.load(model_path)\n",
    "        self.model.load_state_dict(network_state_dict)\n",
    "        optimizer_state_dict = torch.load(optimizer_path)\n",
    "        self.optimizer.load_state_dict(optimizer_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
